# DebateGraph â€” Real-Time Argumentative Analysis Engine

*From Speech to Structured Logic*

**Alexandre** | February 2026 | v0.2 â€” Design Document

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Architecture Globale](#2-architecture-globale)
3. [Stack Technique & Choix Technologiques](#3-stack-technique--choix-technologiques)
4. [Features DÃ©taillÃ©es](#4-features-dÃ©taillÃ©es)
5. [Visualisation Dynamique](#5-visualisation-dynamique)
6. [StratÃ©gie de DÃ©veloppement](#6-stratÃ©gie-de-dÃ©veloppement)
7. [DiffÃ©renciation et Positionnement](#7-diffÃ©renciation-et-positionnement)
8. [Configuration et Secrets](#8-configuration-et-secrets)
9. [Extensions Futures](#9-extensions-futures)

---

## 1. Executive Summary

DebateGraph est un moteur d'analyse argumentative temps rÃ©el qui transforme un flux audio de dÃ©bat en un graphe logique interactif. Le systÃ¨me combine la reconnaissance vocale (speaker diarization + STT), l'extraction d'arguments via LLM, la dÃ©tection automatisÃ©e de fallacies, le fact-checking asynchrone, et une visualisation dynamique sous forme de graphe orientÃ©.

L'objectif n'est pas de remplacer le jugement humain mais d'agir comme un **co-pilote Ã©pistÃ©mique** : le systÃ¨me structure, vÃ©rifie et interroge les arguments, tout en laissant l'utilisateur maÃ®tre de son interprÃ©tation.

**Positionnement :** L'Ã©cosystÃ¨me actuel est fragmentÃ© entre des architectures industrielles massives (IBM Project Debater), des outils de visualisation lÃ©gers (Argdown, Kialo), et des rÃ©seaux de fact-checking participatifs (CaptainFact). DebateGraph intÃ¨gre ces trois dimensions dans un pipeline unifiÃ©, portable et open-source.

---

## 2. Architecture Globale

Le systÃ¨me suit une architecture multi-agents Ã©vÃ©nementielle, inspirÃ©e des frameworks OWL/CAMEL-AI et du standard MCP (Model Context Protocol). PlutÃ´t qu'un modÃ¨le monolithique, chaque responsabilitÃ© est dÃ©lÃ©guÃ©e Ã  un agent spÃ©cialisÃ© orchestrÃ© par un routeur d'intention central.

### 2.1 Pipeline principal : Audio â†’ Argument Graph â†’ Analyse

Le pipeline se dÃ©compose en cinq Ã©tapes sÃ©quentielles. Deux modes d'entrÃ©e sont supportÃ©s : **microphone** (temps rÃ©el) et **import de fichier** (batch). La logique d'analyse est identique dans les deux cas ; seul le mÃ©canisme d'ingestion diffÃ¨re.

| # | Ã‰tape | Input â†’ Output | Tech |
|---|-------|----------------|------|
| 1 | Speaker Diarization | Audio brut â†’ Segments timestampÃ©s par speaker | pyannote/community-1 (pyannote.audio 4.0) |
| 2 | Speech-to-Text | Segments audio â†’ Texte transcrit par speaker | faster-whisper `large-v3-turbo` (local) |
| 3 | Claim Extraction | Texte transcrit â†’ Claims unitaires typÃ©s (prÃ©misse, conclusion, concession, rÃ©futation) | Claude API (structured outputs) |
| 4 | Graph Construction | Claims â†’ Graphe orientÃ© avec arÃªtes typÃ©es (supporte, contredit, reformule, implique) | NetworkX + Claude API pour infÃ©rence des relations |
| 5 | Multi-Agent Analysis | Graphe â†’ Annotations (fallacies, fact-checks, scores, cycles) | Agents spÃ©cialisÃ©s (voir Â§4) |

**Choix architectural clÃ© :** contrairement Ã  IBM Project Debater qui utilisait un pipeline sÃ©quentiel en cascade (segmentation â†’ classification â†’ relation), notre approche privilÃ©gie un modÃ¨le end-to-end pour les Ã©tapes 3â€“4. Le LLM reÃ§oit un segment de transcription complet et produit simultanÃ©ment les claims, leurs types, et les relations inter-claims. Cela Ã©vite le problÃ¨me de propagation d'erreurs en cascade documentÃ© dans la littÃ©rature.

#### Mode fichier vs. mode microphone

```
Mode fichier :
  [Upload fichier audio/vidÃ©o]
       â†“
  [ffmpeg â†’ WAV 16kHz mono]
       â†“
  [WhisperX : diarization + STT en un seul pass]
       â†“
  [Pipeline d'analyse (Ã©tapes 3â€“5)]
       â†“
  [Graphe complet + visualisation avec waveform playback]

Mode microphone :
  [Web Audio API â†’ chunks 5s avec overlap]
       â†“ (WebSocket)
  [faster-whisper streaming + pyannote VAD]
       â†“
  [Pipeline d'analyse (Ã©tapes 3â€“5) en continu]
       â†“
  [Graphe mis Ã  jour en temps rÃ©el]
```

### 2.2 Architecture multi-agents

Un routeur d'intention central distribue de maniÃ¨re asynchrone les claims extraits vers un consortium de quatre agents spÃ©cialisÃ©s. Chaque agent opÃ¨re indÃ©pendamment et Ã©crit ses annotations dans le graphe partagÃ© via une file Redis Streams.

- **Agent Ontologique (Structurel) :** responsable de la conversion des claims en nÅ“uds AIF (Argument Interchange Format), de l'identification des Argumentative Discourse Units et de la construction du graphe d'ancrage infÃ©rentiel. C'est le seul agent qui modifie la topologie du graphe.

- **Agent Sceptique (Fallacy Hunter) :** modÃ¨le fine-tunÃ© (~120M paramÃ¨tres) sur le corpus LOGIC (2 449 Ã©chantillons, 13 types de fallacies). Agit comme moniteur de runtime rapide. Ne dÃ©clenche un LLM gÃ©nÃ©ratif (plus lent, plus prÃ©cis) que lors de la dÃ©tection d'une anomalie, pour prÃ©server la latence globale.

- **Agent Chercheur (Fact-Checker) :** pour chaque claim factuel (distinguÃ© des claims d'opinion par l'Agent Ontologique), lance une recherche asynchrone via web search API. Retourne un verdict structurÃ© avec sources. OpÃ¨re de maniÃ¨re dÃ©couplÃ©e du pipeline principal pour ne pas bloquer le temps rÃ©el.

- **Agent Prosodique (Emotion Analyzer) :** analyse le signal audio brut (pas le texte) pour extraire les dimensions para-verbales : ton, dÃ©bit, micro-hÃ©sitations, marqueurs de sarcasme. CorrÃ¨le ces signaux avec les claims pour dÃ©tecter les appels Ã  l'Ã©motion fallacieux. *Phase 4 uniquement.*

---

## 3. Stack Technique & Choix Technologiques

### 3.1 Speech-to-Text : faster-whisper `large-v3-turbo`

**Benchmark de rÃ©fÃ©rence (Artificial Analysis AA-WER v2.0, fÃ©v. 2025) :**

| ModÃ¨le | WER (AA-WER) | Speed Factor | Prix/1000 min |
|--------|-------------|-------------|---------------|
| Whisper Large v2 (OpenAI) | **4.2%** | 29x | $6.00 |
| Whisper Large v3 Turbo (Fireworks) | **4.8%** | 442x | $1.00 |
| Whisper Large v3 Turbo (Groq) | **4.8%** | 375x | $0.67 |
| Whisper Large v3 (fal.ai) | 4.3% | 82x | $1.15 |
| NVIDIA Canary Qwen 2.5B (open-source #1) | 5.63% WER* | 418x | gratuit |
| GPT-4o Transcribe (meilleur cloud) | ~3.5% | variable | $6+ |

*WER sur le HuggingFace Open ASR Leaderboard, non comparable directement Ã  AA-WER.*

**DÃ©cision : `faster-whisper` avec le modÃ¨le `large-v3-turbo` en local.**

Justification : le modÃ¨le `large-v3-turbo` d'OpenAI offre 4.8% WER (quasi-identique Ã  `large-v3` pour ~2x le speed factor) et fonctionne nativement avec faster-whisper via CTranslate2, ce qui permet une infÃ©rence locale trÃ¨s rapide (GPU optionnel mais recommandÃ©). Pour le mode temps rÃ©el, chunking par fenÃªtres de 5s avec overlap de 1s.

Pour la phase 1 (MVP offline), on utilise Ã©galement **WhisperX** qui intÃ¨gre faster-whisper + diarization pyannote dans un pipeline unifiÃ©, simplifiant l'Ã©tape 1+2 en une seule commande.

### 3.2 Speaker Diarization : pyannote/speaker-diarization-community-1

**DÃ©cision : `pyannote/speaker-diarization-community-1` (pyannote.audio 4.0)**

Sorti dÃ©but 2025, `community-1` est le meilleur modÃ¨le open-source de diarization disponible, outperformant `speaker-diarization-3.1` sur toutes les mÃ©triques clÃ©s (rÃ©duction significative du speaker confusion rate, amÃ©lioration du speaker counting). Il est gratuit, auto-hÃ©bergÃ©, et compatible GPU/CPU. `pyannote-3.1` reste la rÃ©fÃ©rence dans la littÃ©rature mais `community-1` le surpasse dÃ©sormais sans contrepartie.

Pour le mode fichier : **WhisperX** wrape les deux (faster-whisper + pyannote community-1) de maniÃ¨re optimale et gÃ¨re la rÃ©conciliation des timestamps STT/diarization, un problÃ¨me non trivial documentÃ© par l'Ã©quipe pyannote.

### 3.3 Frontend : React + Vite + TypeScript

**DÃ©cision : React + Vite (TypeScript) avec Tailwind CSS.**

Alternatives considÃ©rÃ©es :

| Option | Avantages | InconvÃ©nients | Verdict |
|--------|-----------|---------------|---------|
| React + Vite | Standard industrie, Ã©cosystÃ¨me riche, Cytoscape-React natif, WaveSurfer.js, vibe-coding facile | â€” | âœ… Retenu |
| Svelte/SvelteKit | LÃ©ger, DX agrÃ©able | Moins mainstream pour portfolio, moins d'intÃ©grations natives | âŒ |
| Vue | Bon DX | Moins reconnu en ML/data engineering | âŒ |
| Vanilla JS | Aucune dÃ©pendance | TrÃ¨s verbeux pour la complexitÃ© UI requise | âŒ |

**Justification :**
- **Cytoscape.js** dispose d'un wrapper React officiel (`react-cytoscapejs`) â†’ graphe argumentatif natif
- **WaveSurfer.js** s'intÃ¨gre proprement en React â†’ waveform audio interactive pour le mode fichier
- **Web Audio API** est accessible depuis n'importe quel framework JS â†’ capture microphone
- **Tailwind CSS** â†’ libertÃ© totale sur l'UI/UX, vibe-codable
- Build statique via `vite build` â†’ dÃ©ploiement trivial (Nginx, GitHub Pages, etc.)
- Reconnaissance immÃ©diate sur un portfolio GitHub

### 3.4 Stack technique complÃ¨te

| Composant | Technologie | Justification |
|-----------|-------------|---------------|
| **STT (local)** | `faster-whisper` `large-v3-turbo` | WER 4.8%, 375â€“442x speed factor, CTranslate2 optimisÃ© |
| **STT + Diarization (fichier)** | WhisperX | Pipeline unifiÃ©, rÃ©conciliation timestamps automatique |
| **Diarization** | `pyannote/speaker-diarization-community-1` | Meilleur open-source 2025, outperforme 3.1 |
| **LLM principal** | Claude API (`claude-opus-4` ou `claude-sonnet-4`) | Extraction de claims, infÃ©rence de relations, structured outputs |
| **Classifieur fallacies** | ModÃ¨le fine-tunÃ© (~120M params) sur corpus LOGIC | Classifieur rapide, ne dÃ©clenche LLM que sur anomalie |
| **Graph store** | NetworkX (in-memory) â†’ Neo4j (persistance v2) | NetworkX suffit pour un dÃ©bat unitaire |
| **Fact-checking** | Agent LangGraph + web search API (Tavily ou SerpAPI) | Workflow agentic dÃ©couplÃ© (async) |
| **Analyse prosodique** | Empath + rÃ©seaux acoustiques custom | Phase 4 uniquement |
| **Backend** | FastAPI + WebSocket natif | Endpoints REST + streaming temps rÃ©el vers le front |
| **Message broker** | Redis Streams (v0) â†’ Kafka (v2) | LÃ©ger pour le proto, scalable pour la prod |
| **Frontend** | React + Vite + TypeScript + Tailwind | Standard industrie, libertÃ© UI/UX, vibe-coding |
| **Graphe 2D** | Cytoscape.js (`react-cytoscapejs`) | Graphe interactif, force-directed, bien documentÃ© |
| **Waveform audio** | WaveSurfer.js | Visualisation de la piste audio pour le mode fichier |
| **Capture microphone** | Web Audio API natif | Chunks streaming vers le backend via WebSocket |
| **Graphe 3D (v2)** | Three.js / Cosmograph | Phase visualisation avancÃ©e |
| **Transport** | WebSocket natif | Event-driven, mise Ã  jour dynamique du graphe |

---

## 4. Features DÃ©taillÃ©es

### 4.1 DÃ©tection de fallacies

La littÃ©rature montre que les LLMs standard performent de maniÃ¨re mÃ©diocre sur la classification de fallacies (scores micro-F1 entre 8,62% et 53,31% sur le benchmark LOGIC). Notre approche combine un classifieur rapide spÃ©cialisÃ© avec un LLM gÃ©nÃ©ratif contextuel pour maximiser le compromis latence/prÃ©cision.

**MÃ©thode de distillation structurelle :** suivant les modÃ¨les structure-aware de la littÃ©rature, l'Agent Sceptique identifie les segments sÃ©mantiquement similaires dans un argument, les masque, puis transmet ces instances masquÃ©es au classifieur. Cela force le modÃ¨le Ã  se concentrer sur le pattern de raisonnement sous-jacent plutÃ´t que sur les mots-clÃ©s de surface.

| Fallacie | MÃ©canisme de dÃ©tection | DifficultÃ© algorithmique |
|----------|----------------------|--------------------------|
| **Strawman** | Comparaison d'embeddings entre le claim original (Speaker A) et la reformulation par Speaker B. Score cosinus faible + intent rÃ©futatif = flag. | **Ã‰levÃ©e.** NÃ©cessite un historique parfait de la prÃ©misse originale + mesure de divergence sÃ©mantique + dÃ©termination d'intentionnalitÃ©. |
| **Goal-post Moving** | Tracking temporel des win conditions par speaker et par topic. DÃ©tection de redÃ©finition des critÃ¨res aprÃ¨s satisfaction. | **Ã‰levÃ©e.** Distinction entre argument multi-propositionnel lÃ©gitime et fuite discursive ad hoc. |
| **Raisonnement circulaire** | DÃ©tection de cycles dans le graphe orientÃ© (DFS). Mapping prÃ©misses/conclusion dans un mÃªme espace vectoriel pour vÃ©rifier la prÃ©supposition mutuelle. | **Moyenne.** La dÃ©tection de cycle est triviale algorithmiquement (O(V+E)) ; l'interprÃ©tation sÃ©mantique ne l'est pas. |
| **Ad Hominem** | Classification NER + dÃ©tection d'attaque dirigÃ©e vers une personne plutÃ´t que vers un argument. | **Faible Ã  moyenne.** Pattern lexical relativement stable. |
| **Slippery Slope** | DÃ©tection de chaÃ®nes causales non justifiÃ©es : A â†’ B â†’ C â†’ catastrophe, oÃ¹ les liens intermÃ©diaires manquent de support factuel. | **Moyenne.** Requiert l'Ã©valuation de la force de chaque lien causal de la chaÃ®ne. |
| **Appeal to Emotion** | CorrÃ©lation entre l'analyse prosodique (Agent Prosodique) et l'absence de support factuel. Cadrage Ã©motionnel sans substance logique. | **Ã‰levÃ©e.** Le cadrage Ã©motionnel rÃ©duit de 14,5% la capacitÃ© humaine Ã  dÃ©tecter les fallacies (littÃ©rature). |
| **False Dilemma** | DÃ©tection de structure binaire artificielle ("soit A, soit B") quand le LLM identifie des alternatives viables non mentionnÃ©es. | **Moyenne.** NÃ©cessite des connaissances du domaine pour identifier les alternatives. |

### 4.2 Fact-checking asynchrone

L'Agent Chercheur opÃ¨re de maniÃ¨re dÃ©couplÃ©e du pipeline principal pour ne pas bloquer le temps rÃ©el. Les rÃ©sultats apparaissent progressivement sur le graphe (badges vert/rouge/gris par nÅ“ud).

**Workflow :**

1. L'Agent Ontologique tag chaque claim comme factuel ou d'opinion.
2. Pour chaque claim factuel, l'Agent Chercheur reformule en query vÃ©rifiable.
3. Recherche via web search API (sources prioritaires : articles scientifiques, sites institutionnels, bases de donnÃ©es officielles).
4. Retourne un verdict structurÃ© : `{ verdict, confidence, sources[], contradicting_evidence[] }`.
5. Le verdict est injectÃ© comme annotation sur le nÅ“ud correspondant du graphe.

**Verdicts possibles :** `supported` | `refuted` | `unverifiable` | `partially_true`

**Inspiration CaptainFact :** en plus du fact-checking automatisÃ©, le systÃ¨me pourrait exposer une API permettant Ã  des utilisateurs humains de sourcer, confirmer ou rÃ©futer des claims, crÃ©ant un modÃ¨le hybride human-in-the-loop. Cette couche collaborative est optionnelle mais renforce la confiance dans les verdicts.

### 4.3 DÃ©tection de strawman

Quand Speaker B "rÃ©pond" Ã  Speaker A, le systÃ¨me vÃ©rifie que B rÃ©pond rÃ©ellement Ã  ce que A a dit. La littÃ©rature dÃ©finit le strawman par la prÃ©sence simultanÃ©e de deux dimensions : *misrepresentation aspect* (dÃ©formation du contenu) et *refutational aspect* (utilisation de cette dÃ©formation comme base d'attaque).

**ImplÃ©mentation technique :** on compare l'embedding du claim original de A avec la reformulation implicite que B en fait. Un score de similaritÃ© cosinus infÃ©rieur Ã  un seuil calibrÃ© (typiquement 0.7â€“0.8 selon le domaine), combinÃ© Ã  la dÃ©tection d'intent rÃ©futatif, dÃ©clenche un flag strawman que le LLM confirme ou infirme par analyse contextuelle.

### 4.4 DÃ©tection de goal-post moving

Le systÃ¨me track les claims d'un mÃªme speaker sur un mÃªme topic au fil du temps. Si la thÃ¨se initiale mute silencieusement (les critÃ¨res de succÃ¨s changent sans acknowledgment), le systÃ¨me le flag. C'est du tracking de drift sÃ©mantique par topic et par speaker.

**Formellement :** le systÃ¨me cartographie les win conditions initiales du dÃ©bat. Si un nÅ“ud de rÃ©futation logique est validÃ© par les preuves apportÃ©es, mais que le dÃ©batteur gÃ©nÃ¨re un nouveau nÅ“ud de conditionnalitÃ© (Issue B, puis Issue C) sans jamais concÃ©der le nÅ“ud prÃ©cÃ©dent, le glissement ad hoc est identifiÃ©.

### 4.5 DÃ©tection de cycles et circularitÃ©

DÃ©tection algorithmique directe : recherche de cycles dans le graphe orientÃ© par DFS standard (complexitÃ© O(V+E)). La vraie valeur est dans l'interprÃ©tation : quand un cycle A â†’ B â†’ C â†’ A est dÃ©tectÃ©, le LLM explique en langage naturel pourquoi c'est circulaire et quel nÅ“ud nÃ©cessite une justification externe pour "casser" le cycle.

**Connexion thÃ©orique :** la littÃ©rature sur le model checking (algorithmes Assume-Guarantee, rÃ¨gle CIRC-AG) fournit un cadre formel pour traiter le raisonnement circulaire. L'algorithme ACR gÃ©nÃ¨re itÃ©rativement des contraintes d'appartenance en utilisant des solveurs SAT pour affiner les hypothÃ¨ses. Ce cadre peut Ãªtre adaptÃ© pour la vÃ©rification formelle des chaÃ®nes argumentatives.

### 4.6 Score de rigueur par participant

Un score composite affichÃ© en temps rÃ©el, basÃ© sur :

- Ratio claims supportÃ©s / non-supportÃ©s
- Nombre de fallacies dÃ©tectÃ©es (pondÃ©rÃ© par gravitÃ©)
- Taux de fact-check positif
- CohÃ©rence interne (contradictions entre ses propres claims)
- Taux de rÃ©ponse directe aux arguments adverses vs esquive
- Score prosodique (ratio factuel/Ã©motionnel) â€” Phase 4

### 4.7 Analyse prosodique et multimodalitÃ© *(Phase 4)*

La majoritÃ© des systÃ¨mes d'analyse discursive se limitent au texte transcrit. Or, une proportion significative du discours fallacieux repose sur des signaux para-verbaux : modulations tonales, micro-hÃ©sitations, expressions de sarcasme, variations de dÃ©bit. La littÃ©rature montre que le cadrage Ã©motionnel gÃ©nÃ©rÃ© par l'IA rÃ©duit la capacitÃ© humaine de dÃ©tection de 14,5%.

L'Agent Prosodique extrait directement du signal audio (pas du texte) des features Ã©motionnelles via des embeddings acoustiques. La bibliothÃ¨que Empath (200+ catÃ©gories) et des rÃ©seaux de neurones acoustiques modÃ©lisent les niveaux de colÃ¨re, peur, tentative de tromperie. Ces tenseurs sont fusionnÃ©s avec les reprÃ©sentations nodales du graphe pour produire un score de crÃ©dibilitÃ© asymÃ©trique par claim.

**Use case clÃ© :** corrÃ©ler l'apparition d'un pic Ã©motionnel (dÃ©tectÃ© par l'audio) avec un claim sans support factuel (dÃ©tectÃ© par le graphe) pour flagger un Appeal to Emotion.

### 4.8 Mode modÃ©rateur

Le systÃ¨me gÃ©nÃ¨re en temps rÃ©el des suggestions pour un modÃ©rateur humain :

- "Speaker A n'a pas rÃ©pondu au point X de Speaker B (posÃ© Ã  12:32)"
- "Le claim Y de Speaker B n'a pas Ã©tÃ© sourcÃ© et contredit le consensus scientifique sur Z"
- "Contradiction dÃ©tectÃ©e entre ce que Speaker A a dit Ã  5:32 et ce qu'il affirme maintenant"
- "Le dÃ©bat a dÃ©viÃ© du sujet initial depuis 3 minutes. Sujet courant : X. Sujet original : Y"

### 4.9 Co-pilote Ã©pistÃ©mique : l'inoculation psychologique

L'Ã©cueil des outils de fact-checking actuels est leur posture punitive : ils signalent "faux" ou "illogique", dÃ©clenchant une dissonance cognitive chez l'utilisateur. En s'appuyant sur la thÃ©orie de l'inoculation (Inoculation Theory, illustrÃ©e par des projets comme Bad News ou FallacyCheck), DebateGraph adopte une posture socratique.

**ConcrÃ¨tement :** lorsqu'une fallacie de type Slippery Slope est dÃ©tectÃ©e, au lieu d'afficher *"Fallacie dÃ©tectÃ©e : pente glissante"*, le systÃ¨me formule une question contextuelle : *"Les donnÃ©es prÃ©sentÃ©es permettent-elles d'Ã©tablir un lien de causalitÃ© inÃ©vitable entre A et Z, ou d'autres variables peuvent-elles intervenir ?"*

L'utilisateur peut choisir entre le mode **"juge"** (alertes directes) et le mode **"socratique"** (questions guidÃ©es).

### 4.10 RÃ©sumÃ© structurÃ© post-dÃ©bat

Un rapport auto-gÃ©nÃ©rÃ© exportable en PDF/Markdown contenant : thÃ¨se de chaque camp, meilleurs arguments de chaque cÃ´tÃ©, points non rÃ©solus, fact-checks, fallacies dÃ©tectÃ©es, score de rigueur final, et la structure complÃ¨te du graphe argumentatif.

---

## 5. Visualisation Dynamique

### 5.1 ModÃ¨le de graphe

Le graphe suit la sÃ©mantique colorimÃ©trique d'Argdown adaptÃ©e :

| Type d'arÃªte | Couleur | SÃ©mantique |
|--------------|---------|-----------|
| Soutien | ğŸŸ© Vert | A fournit une Ã©vidence ou un raisonnement qui renforce B |
| Attaque | ğŸŸ¥ Rouge | A contredit directement B ou prÃ©sente une Ã©vidence inverse |
| Sape (Undercut) | ğŸŸª Violet | A conteste le lien logique entre B et C (pas B lui-mÃªme) |
| Reformulation | â¬œ Gris | A et B expriment la mÃªme idÃ©e diffÃ©remment (classe d'Ã©quivalence) |
| Implication | ğŸŸ¦ Bleu | A implique logiquement B (consÃ©quence nÃ©cessaire) |

Chaque nÅ“ud porte des annotations visuelles :

- **Badge fact-check :** âœ“ (vert) | âœ— (rouge) | ? (gris = en cours / unverifiable)
- **Halo de fallacie :** bordure rouge clignotante avec label
- **Indicateur de speaker :** couleur de fond distincte par participant
- **Score de confiance :** opacitÃ© du nÅ“ud proportionnelle au score de confiance du claim

### 5.2 Mode fichier : waveform synchronisÃ©e

Quand l'utilisateur importe un fichier audio/vidÃ©o, le frontend affiche une waveform interactive (WaveSurfer.js) synchronisÃ©e avec le graphe. La lecture audio avance en temps rÃ©el et le graphe s'anime au rythme de la transcription originale (replay pas-Ã -pas). On peut cliquer sur n'importe quel nÅ“ud du graphe pour sauter Ã  l'instant correspondant dans l'audio, et vice-versa.

### 5.3 Interaction et Ã©dition

Contrairement aux systÃ¨mes passifs (OVA, Rationale), DebateGraph permet l'Ã©dition Ã  la volÃ©e :

- Cliquer sur un nÅ“ud pour rÃ©viser l'interprÃ©tation de l'IA via un prompt local
- GÃ©nÃ©rer un sous-arbre conditionnel ("que se passerait-il si cette prÃ©misse Ã©tait fausse ?")
- Forcer une reclassification asynchrone du graphe complet (Collective Classification)
- Annoter manuellement un nÅ“ud (ajout de contexte, sources, commentaires)

### 5.4 Roadmap de visualisation

| Version | Rendu | Tech |
|---------|-------|------|
| v0 | Graphe 2D force-directed, interactif + waveform synchronisÃ©e (fichier) | React + Cytoscape.js + WaveSurfer.js + WebSocket |
| v1 | + Timeline synchronisÃ©e avec l'audio, replay pas-Ã -pas, mode modÃ©rateur UI | + D3.js pour la timeline, synchro audio HTML5 |
| v2 | Graphe 3D immersif, clustering spatial, rendu WebGL | Three.js / Cosmograph / Neo4j NVL |

---

## 6. StratÃ©gie de DÃ©veloppement

### 6.1 Approche incrÃ©mentale

**Principe clÃ© :** commencer par une v0 offline (upload d'un audio/vidÃ©o, analyse aprÃ¨s coup) avant de s'attaquer au temps rÃ©el. Le streaming ajoute Ã©normÃ©ment de complexitÃ© (buffering, latence, synchronisation front/back) et ne change rien Ã  la qualitÃ© de l'analyse.

---

**Phase 1 â€” MVP offline (4â€“6 semaines)**

- Upload audio/vidÃ©o â†’ WhisperX (diarization + transcription) â†’ extraction de claims â†’ construction du graphe
- Visualisation statique du graphe (Cytoscape.js) + waveform synchronisÃ©e (WaveSurfer.js)
- DÃ©tection de fallacies basique (classifieur + LLM)
- Pas de fact-checking, pas de prosodique

---

**Phase 2 â€” Analyse enrichie (4â€“6 semaines)**

- Ajout du fact-checking asynchrone (Agent Chercheur + Tavily/SerpAPI)
- DÃ©tection de cycles, strawman, goal-post moving
- Score de rigueur par participant
- Export du rapport post-dÃ©bat (PDF/Markdown)
- Mode socratique vs. mode juge

---

**Phase 3 â€” Temps rÃ©el (6â€“8 semaines)**

- Streaming microphone â†’ Web Audio API â†’ WebSocket â†’ pipeline complet en temps rÃ©el
- faster-whisper chunking (fenÃªtres 5s, overlap 1s)
- WebSocket pour la mise Ã  jour dynamique du graphe
- Mode modÃ©rateur
- Gestion de la latence et buffering intelligent

---

**Phase 4 â€” MultimodalitÃ© et scaling (8+ semaines)**

- Agent Prosodique (Empath + rÃ©seaux acoustiques)
- Graphe 3D (Three.js/WebGL)
- Persistance Neo4j pour l'analyse multi-dÃ©bats
- API publique et couche collaborative (CaptainFact-like)

### 6.2 Structure du projet

```
debategraph/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                  # FastAPI entrypoint
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ upload.py        # Endpoint upload fichier
â”‚   â”‚   â”‚   â””â”€â”€ ws.py            # WebSocket handler (temps rÃ©el)
â”‚   â”‚   â””â”€â”€ models/              # Pydantic schemas
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ transcription.py     # WhisperX / faster-whisper
â”‚   â”‚   â”œâ”€â”€ diarization.py       # pyannote community-1
â”‚   â”‚   â””â”€â”€ chunker.py           # Streaming audio chunking
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ orchestrator.py      # Routeur d'intention central
â”‚   â”‚   â”œâ”€â”€ ontological.py       # Agent Ontologique
â”‚   â”‚   â”œâ”€â”€ skeptic.py           # Agent Sceptique (fallacies)
â”‚   â”‚   â”œâ”€â”€ researcher.py        # Agent Chercheur (fact-check)
â”‚   â”‚   â””â”€â”€ prosodic.py          # Agent Prosodique (Phase 4)
â”‚   â”œâ”€â”€ graph/
â”‚   â”‚   â”œâ”€â”€ store.py             # NetworkX graph store
â”‚   â”‚   â””â”€â”€ algorithms.py        # DFS cycles, strawman, drift
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ GraphView.tsx    # Cytoscape.js graph
â”‚   â”‚   â”‚   â”œâ”€â”€ WaveformView.tsx # WaveSurfer.js audio
â”‚   â”‚   â”‚   â”œâ”€â”€ FallacyPanel.tsx # DÃ©tails fallacies
â”‚   â”‚   â”‚   â”œâ”€â”€ FactCheckBadge.tsx
â”‚   â”‚   â”‚   â””â”€â”€ RigorScore.tsx
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â”œâ”€â”€ useWebSocket.ts
â”‚   â”‚   â”‚   â””â”€â”€ useAudioCapture.ts
â”‚   â”‚   â”œâ”€â”€ App.tsx
â”‚   â”‚   â””â”€â”€ main.tsx
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.ts
â”œâ”€â”€ .env.example
â”œâ”€â”€ docker-compose.yml           # Redis + backend + frontend
â””â”€â”€ README.md
```

### 6.3 DÃ©bats de test

Pour la validation, privilÃ©gier des dÃ©bats oÃ¹ la vÃ©ritÃ© terrain est facilement Ã©tablissable :

- **DÃ©bats prÃ©sidentiels US/FR :** 2 speakers bien distincts, beaucoup de fallacies, fact-checks existants pour validation croisÃ©e.
- **DÃ©bats Oxford Union :** format structurÃ©, arguments plus rigoureux, bon test pour la dÃ©tection fine.
- **Podcasts dÃ©bat (Lex Fridman, Intelligence Squared) :** conversations longues, multi-topics, bon stress-test pour le tracking de drift.

---

## 7. DiffÃ©renciation et Positionnement

| Dimension | Ã‰tat de l'art | DebateGraph |
|-----------|--------------|-------------|
| **Architecture** | Pipeline sÃ©quentiel ou LLM monolithique | Multi-agents spÃ©cialisÃ©s + routeur d'intention |
| **Analyse** | Texte seul (post-transcription) | Multimodal : texte + prosodie + Ã©motion audio |
| **Fallacies** | Classification simple, scores F1 faibles | Classifieur rapide + LLM contextuel + distillation structurelle |
| **Visualisation** | Post-hoc, statique (OVA, Rationale) | Temps rÃ©el, interactif, Ã©ditable, 2D/3D + waveform synchronisÃ©e |
| **Fact-checking** | SÃ©parÃ© de l'analyse argumentative | IntÃ©grÃ© au graphe, asynchrone, avec verdicts par nÅ“ud |
| **UX** | Posture punitive ("faux" / "illogique") | Co-pilote Ã©pistÃ©mique socratique (inoculation) |
| **Scope** | Outils fragmentÃ©s en silos | Pipeline unifiÃ© speech-to-graph + analyse + fact-check |

**Le vrai diffÃ©renciateur pour le portfolio** n'est pas la techno (Whisper + LLM, tout le monde peut le faire), c'est la profondeur de l'analyse argumentative. La construction d'un vrai graphe avec dÃ©tection de patterns logiques (cycles, contradictions, strawmen, drift sÃ©mantique) dÃ©montre une comprÃ©hension Ã  la fois du NLP, de la logique formelle, et du software engineering.

---

## 8. Configuration et Secrets

Toutes les clÃ©s et variables d'environnement sont centralisÃ©es dans un fichier `.env` (non commitÃ©). Le fichier `.env.example` est commitÃ© pour documenter les variables attendues.

```bash
# .env.example

# â”€â”€â”€ LLM (Claude API) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ANTHROPIC_API_KEY=

# â”€â”€â”€ Speaker Diarization (pyannote) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CrÃ©er un token en lecture sur https://huggingface.co/settings/tokens
# Accepter les conditions d'utilisation sur :
# https://huggingface.co/pyannote/speaker-diarization-community-1
HUGGINGFACE_TOKEN=

# â”€â”€â”€ Fact-Checking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Option A : Tavily (recommandÃ©, gratuit jusqu'Ã  1000 req/mois)
TAVILY_API_KEY=
# Option B : SerpAPI (alternative)
# SERPAPI_API_KEY=

# â”€â”€â”€ Redis (message broker) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REDIS_URL=redis://localhost:6379

# â”€â”€â”€ Backend â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8000
CORS_ORIGINS=http://localhost:5173

# â”€â”€â”€ Whisper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ModÃ¨le : tiny | base | small | medium | large-v3 | large-v3-turbo
# RecommandÃ© pour dev CPU : medium
# RecommandÃ© pour GPU : large-v3-turbo
WHISPER_MODEL=large-v3-turbo
# Device : cuda | cpu | auto
WHISPER_DEVICE=auto
# Compute type : float16 (GPU) | int8 (CPU)
WHISPER_COMPUTE_TYPE=float16

# â”€â”€â”€ Analyse â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Seuil similaritÃ© cosinus pour la dÃ©tection strawman (0.0â€“1.0)
STRAWMAN_SIMILARITY_THRESHOLD=0.75
# Mode UI par dÃ©faut : judge | socratic
DEFAULT_UI_MODE=socratic
```

---

## 9. Extensions Futures

- **Reconstruction logique formelle :** transformer les arguments en logique propositionnelle/premier ordre et vÃ©rifier la validitÃ© formelle des raisonnements.

- **DÃ©tection de provenance IA :** intÃ©grer une couche de dÃ©tection de contenu gÃ©nÃ©rÃ© par IA (architecture type Pangram) pour certifier l'authenticitÃ© des interlocuteurs dans un contexte de dÃ©bat en ligne.

- **Multilingue :** le pipeline est language-agnostic si le STT et le LLM supportent la langue cible. Le corpus TALN (1600+ articles, 5,8M mots) est une ressource pour le franÃ§ais.

- **API publique :** exposer le pipeline comme un service : envoyer un audio, recevoir un graphe structurÃ© + annotations. Permet l'intÃ©gration dans des apps tierces (mÃ©dias, Ã©ducation, juridique).

- **Gamification Ã©ducative :** inspirÃ© de Bad News (jeu d'inoculation), crÃ©er un mode oÃ¹ l'utilisateur doit identifier les fallacies avant le systÃ¨me. Score d'apprentissage progressif.

- **Key Point Analysis :** dÃ©rivÃ© d'IBM Project Debater, rÃ©sumer un dÃ©bat en un ensemble de points clÃ©s pondÃ©rÃ©s par frÃ©quence de mention. Utile pour l'analyse de dÃ©bats publics massifs (rÃ©seaux sociaux, consultations citoyennes).
